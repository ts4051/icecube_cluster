'''
Tools for submitting to a cluster via the condor system.
Also supports DAGMan.

These tools are generic to any condor system, e.g. do not assume IceCube, NPX, etc.

These can be used standalone, or by the more sophisticated job management system 
defined in `cluster.py` and `job.py`.

Tom Stuttard
'''

import os, collections
from shutil import copy2

from icecube_cluster.utils.filesys_tools import make_dir, is_executable
from icecube_cluster.utils.unix_tools import which, tail

from icecube_cluster.job import JOB_INDEX_FMT, JOB_NAME_FMT

#
# Globals
#

CONDOR_SUBMIT_EXE = "condor_submit_dag"


#
# Submission
#

#TODO These are now coupled to job.py. Iddeally make this generic and all the job.py stuff be contained in cluster.py so that this can be used standalone
#TODO Oeiginals are preserved below

def create_condor_submit_file( 
    file_path, 
    memory_MB=1000,
    disk_space_MB=1000,
    num_cpus=1,
    num_gpus=0,
    export_env=False, 
    require_cvmfs=False, 
    require_avx=False,
    require_cuda=False, 
    require_sl7=False, # Scientific Linux 7 (+ variants)
    choose_sites=None,
    exclude_sites=None,
    user_proxy=False,
    wall_time_hr=None,
    accounting_group=None,
) :
    '''
    Create a condor submit file.
    Assumes it will be run as a DAG, using a DAGMan scriped generated using `create_dagman_submit_file` (below).
    '''

    #TODO Make this more configurable
    #TODO Maybe make it part of the dagman function since it is so tightly coupled

    #TODO https://wiki.icecube.wisc.edu/index.php/Condor/Grid

    # Check inputs
    assert not (choose_sites and exclude_sites), "Cannot specify both `choose_sites` and `exclude_sites`"

    # Make paths absolute
    file_path = os.path.abspath(file_path)

    # Check parent directory exists
    base_dir = os.path.dirname(file_path)
    make_dir(base_dir)

    # Make file
    with open(file_path,"w") as submit_file :

        # Write a header
        submit_file.write( "# Autogenerated using fridge/utils/cluster/condor.create_condor_submit_file\n" )

        # Write executable line
        submit_file.write( "executable = $(Exe)\n" )

        # Write arguments line
        submit_file.write( "arguments = $(ExeArgs)\n" )

        # Write output streams lines
        submit_file.write("output = $(OutFile)\n" )
        submit_file.write("error = $(ErrFile)\n" )
        submit_file.write("log = $(LogFile)\n" )

        # Set the working directory
        submit_file.write("initialdir = $(WorkingDir)\n" ) #TODO Not sure I actually set this anywhere

        # Write notification line
        submit_file.write( "notification = never\n" )

        # Memory/disk request
        submit_file.write( "request_memory = %iMB\n" % memory_MB )
        submit_file.write( "request_disk = %iMB\n" % disk_space_MB )

        # CPU request
        submit_file.write( "request_cpus = %i\n" % num_cpus )

        # GPU request
        # Usage guide: https://wiki.icecube.wisc.edu/index.php/Condor#GPU_usage
        # Note that should not include `request_gpus = 0` if don't need any, as on the IceCube grid this 
        # results in the following property`(TARGET.gpus >= Requestgpus)` being set, which means that 
        # clusters not delcaring the `gpus` classad won't be allocated my jobs, even though I don't need 
        # GPUs (thanks vbrik for the info)
        if num_gpus > 0 :
            submit_file.write( "request_gpus = %i\n" % num_gpus )

        # Export the environment of the submitter if the user requested it
        # WARNING : This can be dangerous, and is not recommended (particularly if using GPUs)
        if export_env :
            submit_file.write("getenv = True\n")

        # Copy local files back across in the end
        submit_file.write("should_transfer_files = YES\n") #TODO Think more about this, there are various option

        # Build a requirements list
        requirements = []
        if require_cvmfs :
            requirements.append("HAS_CVMFS_icecube_opensciencegrid_org")
        if require_avx :
            requirements.append("Has_AVX")
        if require_cuda :
            requirements.append("CUDACapability")
        if require_sl7 :
            requirements.append("( OpSysAndVer == \"SL7\" || OpSysAndVer == \"CentOS7\" )")
        if (choose_sites is not None) and (len(choose_sites) > 0) :
            requirements.append("( " + (" || ".join([ "(GLIDEIN_Site == \"%s\")"%site for site in choose_sites ]) + " )"))
        if (exclude_sites is not None) and (len(exclude_sites) > 0) :
            requirements.append("( " + (" && ".join([ "(GLIDEIN_Site =!= \"%s\")"%site for site in exclude_sites ]) + " )"))
        if len(requirements) > 0 :
            submit_file.write('Requirements = ( %s )\n' % (" && ".join(requirements)) )

        # Handle user certification for remote sites where this is required
        if user_proxy :
            submit_file.write('use_x509userproxy=true\n')

        # Define a walltime
        if wall_time_hr is not None :
            submit_file.write('+OriginalTime = %i\n' % (wall_time_hr*3600) ) # Madison ClassAd. Unit is [s]. Not sure how many sites support this, but works at least for MSU and doesn't seem to break anything elsewhere
            #TODO are there other ways to specify a walltime? e.g. other grid sites
            #TODO NPX (which has special queues)

        # Add accounting group if requested
        if accounting_group is not None:
            submit_file.write('+AccountingGroup = \"%s.$ENV(USER)\"\n'%accounting_group)

        # Write queue line
        submit_file.write( "queue\n" )

        print(("Condor submit file written : %s" % (file_path)))
        return file_path



#Create a DAGMan submit file
def create_dagman_submit_file(
    submit_dir,
    log_dir,
    dagman_file_name,
    condor_file_name,
    jobs,
    **submit_file_kw
) :

    #TODO Remove dependence on job.py (instead make this generic and put the job.py dependence in cluster.py)

    #
    # Handle inputs
    #

    # Check the submission file dir exists
    submit_dir = os.path.expandvars( os.path.expanduser(submit_dir) )
    submit_dir = os.path.abspath(submit_dir)
    assert os.path.isdir(submit_dir), "Cannot create DAGMan file, submission file directory \"%s\" does not exist" % submit_dir

    # Form paths for submit files to be generated
    dagman_file_path = os.path.abspath( os.path.join(submit_dir,dagman_file_name) )
    condor_file_path = os.path.abspath( os.path.join(submit_dir,condor_file_name) )


    #
    # Create submit files
    #

    #Create the submit file this DAGMan file will use
    create_condor_submit_file( condor_file_path, **submit_file_kw )

    # Make file
    with open(dagman_file_path,"w") as dagman_file :

        # Write a header
        dagman_file.write( "# Autogenerated using fridge/utils/cluster/condor.create_dagman_submit_file\n" )

        # Loop over jobs provided (each one is a list of args)
        for job in jobs :

            # Write job line
            job_name = JOB_NAME_FMT % job.index
            dagman_file.write( "JOB %s %s\n" % (job_name, condor_file_path) )

            # Break command up in to exe/args
            # Depends on whether an env shell is provided, and whether an exe script (e.g. a job wrapper script) is provided
            if job.wrapper_script is None :
                assert len(job.commands) == 1, "'No wrapper mode' only supports single commands, no bunching"
                exe_path = job.commands[0].command
                # exe_args = ""
            else :
                exe_path = job.wrapper_script
                # exe_args = ""

            # Check executable exists and is executable
            assert os.path.isfile(exe_path), "Executable file does not exist : %s" % exe_path
            assert is_executable(exe_path), "Executable file is not executable (change the permissions) : %s" % exe_path

            # Define log file
            # This can hammer a file system, so normally want this to be scratch
            log_file = os.path.join( log_dir, JOB_INDEX_FMT%job.index + ".log" )

            # Define generic args used for all jobs
            arg_string = "JobIndex=\"%s\"" % (JOB_INDEX_FMT%job.index)
            arg_string += " Exe=\"%s\"" % (exe_path)
            # arg_string += " ExeArgs=\"%s\"" % (exe_args)
            arg_string += " OutFile=\"%s\"" % (job.out_file)
            arg_string += " ErrFile=\"%s\"" % (job.err_file)
            arg_string += " LogFile=\"%s\"" % (log_file)

            # Write the args for the job
            dagman_file.write( "VARS %s %s\n" % (job_name,arg_string) )

    #Done
    print(("DAGMan submit file written : %s" % (dagman_file_path)))
    return dagman_file_path,condor_file_path



# #Create a submit file for condor
# #Assumes arguments are going to passed from a DAGMan submit file, so this is generally
# #called from create_dagman_submit_file
# def create_condor_submit_file(file_path,arg_names=None,export_env=False) :

#     # Initialise list of args
#     #TODO Needs refurbishing
# #    if arg_names == None : 
# #        arg_names = []

#     #Make paths absolute
#     file_path = os.path.abspath(file_path)

#     #Check parent directory exists
#     base_dir = os.path.dirname(file_path)
#     make_dir(base_dir)

#     # Make file
#     with open(file_path,"w") as submit_file :

#         # Write a header
#         submit_file.write( "# Autogenerated using fridge/utils/cluster/condor.create_condor_submit_file\n" )

#         # Write executable line
#         submit_file.write( "executable = $(Exe)\n" )

#         # Write arguments line
#         submit_file.write( "arguments = $(ExeArgs)\n" )

#         # Write arguments line, if user provided any
#         #TODO Needs refurbishing
#         # if len(arg_names) > 0 : 
#         #     submit_file.write( "Arguments = %s\n" % " ".join([ "$(%s)" % a for a in arg_names ]) )

#         # Write output streams lines
#         output_file_stem = "job_$(JobIndex)__$(Cluster)"
#         submit_file.write("output = $(SubmitDir)/%s.out\n" % (output_file_stem) )
#         submit_file.write("error = $(SubmitDir)/%s.err\n" % (output_file_stem) )
#         submit_file.write("log = $(LogDir)/%s.log\n" % (output_file_stem) )
#         #submit_file.write("initialdir = %s\n" % (base_dir) )

#         # Need to set initial directory somewhere writeable
#         # See https://wiki.icecube.wisc.edu/index.php/Condor/BestPractices#DAGMan
#         # Using absolute paths everything so shouldn't matter
#         #submit_file.write("initialdir = ${HOME}\n" )
#         submit_file.write("initialdir = $(SubmitDir)\n" )

#         # Write notification line
#         submit_file.write( "notification = never\n" )

#         # Memory request
#         submit_file.write( "request_memory = $(Memory)\n" )

#         # GPU request
#         # https://wiki.icecube.wisc.edu/index.php/Condor#GPU_usage
#         submit_file.write( "request_gpus = $(NumGPU)\n" )
#         #TODO Add support for: "requirements = CUDACapability"

#         # Export the environment of the submitter if the user requested it
#         # This can be dangerous, and is not recommended
#         if export_env :
#             submit_file.write("getenv = True\n")

#         # Write queue line
#         submit_file.write( "queue\n" )

#         print "Condor submit file written : %s" % (file_path)
#         return file_path



# #Create a DAGMan submit file
# def create_dagman_submit_file(submit_dir,log_dir,dagman_file_name,condor_file_name,exe_path,exe_args,memory,num_gpus=0,export_env=False) :

#     #
#     # Handle inputs
#     #

#     # Check executable
#     #TODO in principle exe_path could be different for each job, add support for this
#     assert isinstance(exe_path,basestring), "Executable path must be a string"
#     test_exe_path = exe_path
#     if not os.path.isfile(exe_path) :
#         test_exe_path = which(exe_path)
#         if test_exe_path is None :
#             raise Exception("Executable not found : %s" % exe_path)
#     assert is_executable(test_exe_path), "Executable file is not executable (change the permissions) : %s" % exe_path

#     # Check the list of arguments provided
#     # This also defines how many jobs we will run
#     assert isinstance(exe_args,collections.Sequence), "Executable arguments must be a list or similar"
#     for arg_str in exe_args : 
#         assert isinstance(arg_str,basestring), "Each element of the executable arguments list must be a string"
#     num_jobs = len(exe_args)
#     assert num_jobs > 0, "No jobs provided, cannot created DAGMan submit file"

#     # Check the submission file dir exists
#     submit_dir = os.path.expandvars( os.path.expanduser(submit_dir) )
#     submit_dir = os.path.abspath(submit_dir)
#     assert os.path.isdir(submit_dir), "Cannot create DAGMan file, submission file directory \"%s\" does not exist" % submit_dir

#     # Form paths for submit files to be generated
#     dagman_file_path = os.path.abspath( os.path.join(submit_dir,dagman_file_name) )
#     condor_file_path = os.path.abspath( os.path.join(submit_dir,condor_file_name) )


#     #
#     # Create submit files
#     #

#     #Create the submit file this DAGMan file will use
#     create_condor_submit_file(condor_file_path,export_env=export_env)

#     # Make file
#     with open(dagman_file_path,"w") as dagman_file :

#         # Write a header
#         dagman_file.write( "# Autogenerated using fridge/utils/cluster/condor.create_dagman_submit_file\n" )

#         # Loop over jobs provided (each one is a list of args)
#         for i_job,exe_arg_str in enumerate(exe_args) :

#             # Write job line
#             job_index_str = "%08i" % i_job
#             job_id = "job_" + job_index_str
#             dagman_file.write( "JOB %s %s\n" % (job_id,condor_file_path) )

#             # Define generic args used for all jobs
#             arg_string = "JobIndex=\"%s\"" % (job_index_str)
#             arg_string += " Exe=\"%s\"" % (exe_path)
#             arg_string += " ExeArgs=\"%s\"" % (exe_arg_str)
#             arg_string += " Memory=\"%i\"" % (memory)
#             arg_string += " NumGPU=\"%i\"" % (num_gpus)
#             arg_string += " SubmitDir=\"%s\"" % (submit_dir)
#             arg_string += " LogDir=\"%s\"" % (log_dir)

#             # Add any bespoke arguments
#             #TODO This is currently unused and probably needs refurbishing
#             #for i_arg in range(0,len(arg_names)) :
#             #    arg_string += " %s=\"%s\" " % (arg_names[i_arg],job.args[i_arg])

#             # Write the args for the job
#             dagman_file.write( "VARS %s %s\n" % (job_id,arg_string) )

#     #Done
#     print "DAGMan submit file written : %s" % (dagman_file_path)
#     return dagman_file_path,condor_file_path



#
# Parsing metrics
#

#Interpret the DAG status codes (from https://research.cs.wisc.edu/htcondor/manual/latest/2_10DAGMan_Applications.html)
def interpret_dag_status(dag_status) :
    if dag_status == 0 : return "OK"
    elif dag_status == 1 : return "Other error"
    elif dag_status == 2 : return "One or more nodes failed"
    elif dag_status == 3 : return "DAG aborted (ABORT-DAG-ON)"
    elif dag_status == 4 : return "DAG removed (condor_rm)"
    elif dag_status == 5 : return "DAG cycle found"
    elif dag_status == 6 : return "DAG halted"
    else : raise Exception( "Unknown DAG status : %i" % (dag_status) )



#Class for storing DAGMan jobs bunch metrics from the metrics file json node
class DAGManMetrics(object) :

    #Members of DAGManMetrics main class
    def __init__(self,date=None) :
        self.date = date
        self.submit_fileInfo = None
        self.outputFileInfo = None
        self.metricsFileInfo = None
        self.jobsMetrics = list() #TOOD OrderedDict with num?

    def parseSubmitFile(self,dagmanSubmitFilePath) :
        self.submit_fileInfo = DAGManMetricsSubmitFileInfo(dagmanSubmitFilePath)

    def parseOutputFile(self,dagmanOutputFilePath) :
        self.outputFileInfo = DAGManMetricsOutputFileInfo(dagmanOutputFilePath)

    def parseMetricsFile(self,metricsJsonFilePath) :
        self.metricsFileInfo = DAGManMetricsMetricsFileInfo(metricsJsonFilePath)

    def fullyParsed(self) :
        return self.submit_fileInfo != None and self.submit_fileInfo.fullyParsed() and self.outputFileInfo != None and self.outputFileInfo.fullyParsed() and self.metricsFileInfo != None and self.metricsFileInfo.fullyParsed()

    def __str__(self) :
        returnString = "DAGMan submission :\n"
        if self.date != None : 
            returnString += "  Date                  : %s\n" % (self.date)

        if self.submit_fileInfo != None :
            returnString += "  Submit file info :\n"
            returnString += "    Num jobs            : %i\n" % (self.submit_fileInfo.numJobs)
        else :
            returnString += "  [Submit file not parsed]\n"

        if self.outputFileInfo != None :
            returnString += "  Output file info :\n"
            returnString += "    Return code          : %s\n" % ( str(self.outputFileInfo.returnCode) if self.outputFileInfo.returnCode != None else "[Not parsed]" )
        else :
            returnString += "  [Output file not parsed]\n"

        if self.metricsFileInfo != None :
            returnString += "  Metrics file info :\n"
            returnString += "    Num jobs             : %i\n" % (self.metricsFileInfo.numJobs)
            returnString += "    Num jobs succeeded   : %i (%0.3f%%)\n" % (self.metricsFileInfo.numJobsSucceeded,percentage(self.metricsFileInfo.numJobsSucceeded,self.metricsFileInfo.numJobs))
            returnString += "    Num jobs failed      : %i (%0.3f%%)\n" % (self.metricsFileInfo.numJobsFailed,percentage(self.metricsFileInfo.numJobsFailed,self.metricsFileInfo.numJobs))
            returnString += "    Start time           : %s\n" % (self.metricsFileInfo.start_time)
            returnString += "    End time             : %s\n" % (self.metricsFileInfo.end_time)
            returnString += "    Time taken in total  : %s\n" % (time.human_time(self.metricsFileInfo.duration))
            returnString += "    DAG status           : %s\n" % ( interpretDAGStatus(self.metricsFileInfo.dag_status) )
        else :
            returnString += "  [Metrics file not parsed]\n"

        return returnString


#Subclass containing submit file information
class DAGManMetricsSubmitFileInfo(object) :
    
    def __init__(self,dagmanSubmitFilePath) :
        self.parse(dagmanSubmitFilePath)

    def fullyParsed(self) :
        return True #No conditional parsing for this file at this time

    def parse(self,dagmanSubmitFilePath) :

        #Check submit exists, open it, and read it into a string
        if os.path.exists(dagmanSubmitFilePath) :
            with open(dagmanSubmitFilePath) as submit_file :
                submit_fileText = submit_file.read()

                #Count the instances of the "JOB" ketword in the string
                self.numJobs = submit_fileText.count("JOB")

        else :
            raise Exception( "DAGMan submit file \"%s\" does not exist" % dagmanSubmitFilePath )


#Subclass containing metrics file information
class DAGManMetricsMetricsFileInfo(object) :

    def __init__(self,metricsJsonFilePath) :
        self.parse(metricsJsonFilePath)

    def fullyParsed(self) :
        return True #No conditional parsing for this file at this time

    def parse(self,metricsJsonFilePath) :

        #Open the JSON metrics file and parse using a json parser
        if os.path.exists(metricsJsonFilePath) :
            with open(metricsJsonFilePath) as dagmanMetricsFile :

                metricsNode = json.load(dagmanMetricsFile)
                if metricsNode : 

                    #Read individual details from the metrics file
                    self.numJobs = metricsNode['jobs']
                    self.numJobsSucceeded = metricsNode['jobs_succeeded']
                    self.numJobsFailed = metricsNode['jobs_failed']
                    self.start_time = metricsNode['start_time']
                    self.end_time = metricsNode['end_time']
                    self.duration = metricsNode['duration']
                    #self.jobsCombinedDuration = metricsNode['total_job_time'] #TODO replace once DAGman actually writes this variable, currently always 0
                    self.dag_status = metricsNode['dag_status']

                else :
                    raise Exception( "DAGMan metrics file \"%s\" cannot be parsed as a JSON file" % metricsJsonFilePath )

        else :
            raise Exception( "DAGMan metrics file \"%s\" does not exist" % metricsJsonFilePath )


#Subclass containing .out file information
class DAGManMetricsOutputFileInfo(object) :
    
    def __init__(self,dagmanOutputFilePath) :
        self.returnCode = None
        self.parse(dagmanOutputFilePath)

    def parse(self,dagmanOutputFilePath) : #TODO parse more of this file

        #Check submit exists, open it, and read it into a string
        if os.path.exists(dagmanOutputFilePath) :

            #Parse the return status code
            self.returnCode = self.getReturnCode(dagmanOutputFilePath)

        else :
            raise Exception( "DAGMan .out file \"%s\" does not exist" % dagmanOutputFilePath )

    def fullyParsed(self) :
        return self.returnCode != None

    def getReturnCode(self,dagmanOutputFilePath) :

        #Get the last line of the file (which has the status, assuming it has completed)
        statusLines = tail(dagmanOutputFilePath,1)

        #Check got the one line I expected
        if len(statusLines) == 1 :

            statusLine = statusLines[0]

            #Parse status from this line
            statusKey = "EXITING WITH STATUS"
            if statusKey in statusLine :

                #Extract the status key and return it
                statusKeyStartIndex = statusLine.find(statusKey)
                statusCodeStartIndex = statusKeyStartIndex + len(statusKey) + 1 #+1 for the space after it
                if len(statusLine) > ( statusCodeStartIndex + 1 ) :
                    return int( statusLine[statusCodeStartIndex:statusCodeStartIndex+1] )

        #If here, failed in parsing return code (maybe DAG is still running, or died in an uncontrolled way)
        return None



#Class for storing metrics for a given condor job, as parsed from the log file
class CondorJobMetrics(object) :

    #Members of CondorJobMetrics main class
    def __init__(self,num=-1) :
        self.number = num
        self.logFileInfo = None
        self.outFileInfo = None

    def parseLogFile(self,logFilePath) :
        self.logFileInfo = CondorJobMetricsLogFileInfo(logFilePath)

    def parseOutFile(self,outFilePath) :
        self.outFileInfo = CondorJobMetricsOutFileInfo(outFilePath)

    def fullyParsed(self) :
        return self.logFileInfo != None and self.logFileInfo.fullyParsed() and self.outFileInfo != None and self.outFileInfo.fullyParsed()

    def __str__(self) :
        returnString = "Job %s:\n" % ("%i" % self.number if self.number > -1 else "")

        if self.logFileInfo != None :
            returnString += "  Log file info :\n"
            returnString += "    Submit time        : %s\n" % ( self.logFileInfo.submitTime if self.logFileInfo.submitTime != None else "[Not parsed]" )
            returnString += "    Execute time       : %s\n" % ( self.logFileInfo.executeTime if self.logFileInfo.executeTime != None else "[Not parsed]" )
            returnString += "    Terminate time     : %s\n" % ( self.logFileInfo.terminateTime if self.logFileInfo.terminateTime != None else "[Not parsed]" )
            returnString += "    Execution duration : %s\n" % ( self.logFileInfo.getExecuteDuration() if self.logFileInfo.getExecuteDuration() != None else "[Not parsed]" )
            returnString += "    Memory used        : %s\n" % ( "%i [MB]"%self.logFileInfo.memoryUsedMB if self.logFileInfo.memoryUsedMB != None else "[Not parsed]" )
            returnString += "    Memory requested   : %s\n" % ( "%i [MB]"%self.logFileInfo.memoryRequestedMB if self.logFileInfo.memoryRequestedMB != None else "[Not parsed]" )
            returnString += "    Memory allocated   : %s\n" % ( "%i [MB]"%self.logFileInfo.memoryAllocatedMB if self.logFileInfo.memoryAllocatedMB != None else "[Not parsed]" )
            returnString += "    Return code        : %s\n" % ( str(self.logFileInfo.returnCode) if self.logFileInfo.returnCode != None else "[Not parsed]" )
        else :
            returnString += "  [Log file not parsed]\n"

        if self.outFileInfo != None :
            returnString += "  Out file info :\n"
            returnString += "    Num sub jobs       : %s\n" % ( self.outFileInfo.numCommands if self.outFileInfo.numCommands != None else "[Not parsed]" )
        else :
            returnString += "  [Out file not parsed]\n"

        return returnString


#Subclass to be filled with parsed out file
class CondorJobMetricsOutFileInfo(object) :

    def __init__(self,outFilePath) :
        self.numCommands = None
        self.parse(outFilePath)

    def fullyParsed(self) :
        return self.numCommands != None

    def parse(self,outFilePath) :

        #Check file exists, open it, and read into a string
        if os.path.exists(outFilePath) : 
            self.outFilePath = outFilePath
            with open(self.outFilePath) as outFile :
                outFileText = outFile.read()

                #Parse num commands
                self.numCommands = self.getNumCommandsFromFile(outFileText)

    def getNumCommandsFromFile(self,outFileText) :

        #Split text into lines
        lines = outFileText.split('\n')

        #Find num commands line
        key = "JOB INFO : Num commands = "
        for line in lines :
            keyPos = line.find(key)
            if keyPos > -1 :

                #Num commands is rest of line after the key
                return int( line[keyPos+len(key):] )

        #If here, didn't manage to parse
        return None


#Subclass to be filled with parsed log file
class CondorJobMetricsLogFileInfo(object) :

    def __init__(self,logFilePath) :
        self.submitTime = None
        self.executeTime = None
        self.terminateTime = None
        self.memoryUsedMB = None
        self.memoryRequestedMB = None
        self.memoryAllocatedMB = None
        self.returnCode = None
        self.held = None
        self.aborted = None
        self.timeLimitExceeded = None
        self.parse(logFilePath)

    def fullyParsed(self) :
        return self.submitTime != None and self.executeTime != None and self.terminateTime != None and self.memoryUsedMB != None and self.memoryRequestedMB != None and self.memoryAllocatedMB != None and self.returnCode != None 

    def parse(self,logFilePath) :

        #Check file exists, open it, and read into a string
        if os.path.exists(logFilePath) : 
            self.logFilePath = logFilePath
            with open(self.logFilePath) as logFile :

                logFileText = logFile.read()

                #Fill members from info in log file
                #Note that if job file is complete, not all members will be parsed (set to None)

                #self.job_number

                #Parse various times from file
                self.submitTime = self.getTimeFromLineInFile("Job submitted")
                self.executeTime = self.getTimeFromLineInFile("Job executing on host")
                self.terminateTime = self.getTimeFromLineInFile("Job terminated")

                #Handle corner case of job running over a year
                #This is because job log file doesn't include year, so we have to guess it from file modification date
                if self.terminateTime and self.executeTime :
                    if self.executeTime > self.terminateTime : #e.g. if start after end
                        self.terminateTime += datetime.timedelta(days=365.25) #Not perfect but good enough for metrics

                self.memoryUsedMB,self.memoryRequestedMB,self.memoryAllocatedMB = self.getMemoryUsageFromFile(logFileText)

                self.returnCode = self.getReturnCodeFromFile(logFileText)

                #Check if job was held
                self.aborted = "Job was held" in logFileText

                #Check if job was aborted
                self.aborted = "Job was aborted by the user" in logFileText

                #Check if time limit breached
                self.timeLimitExceeded = "Execution time limit exceeded" in logFileText

        else :
            raise Exception("Job log file does not exists")

    def getExecuteDuration(self) :
        if self.terminateTime and self.executeTime :
            return self.terminateTime - self.executeTime
        else :
            return None

    def getTimeFromLineInFile(self,key) :

        #Open the log file
        with open(self.logFilePath) as logFile :

            #Get log file as text (is not too big)
            logFileText = logFile.read()

            #Get time stamp string from file text
            timeStampLength = 14
            pos = logFileText.find(key)
            if pos > -1 : #check found key
                if pos > timeStampLength :
                    timeStamp = logFileText[ pos-timeStampLength : pos ]

                    #Time stamp from condor job log files is annoyingly missing the year, guess it from the file modification date
                    #print time.gmtime(os.path.getmtime(self.logFilePath))
                    #logFileModTime = time.strftime('%m/%d/%Y', time.gmtime(os.path.getmtime(self.logFilePath)) )
                    logFileModTime = datetime.datetime.fromtimestamp(os.path.getmtime(self.logFilePath))
                    timeStamp = str(logFileModTime.year) + "/" + timeStamp

                    #Now parse time stamp
                    parsedTime = datetime.datetime.strptime(timeStamp,"%Y/%m/%d %H:%M:%S ")

                    return parsedTime

        #If here, didn't manage to parse
        return None

    def getReturnCodeFromFile(self,logFileText) :

        #Parse return code from file
        returnCodeLength = 1
        key = "return value "
        keyPos = logFileText.find(key)
        if keyPos > -1 : #check found key
            returnCodePos = keyPos + len(key)
            if len(logFileText) > returnCodePos + 1 :

                #Grad the return code, handling -ve values
                returnCode = logFileText[ returnCodePos : returnCodePos+returnCodeLength ]
                if returnCode == "-" : returnCode += logFileText[ returnCodePos+returnCodeLength : returnCodePos+returnCodeLength+1 ]
                return int(returnCode)

        #If here, didn't manage to parse
        return None

    def getMemoryUsageFromFile(self,logFileText) :

        #Parse memory usage from file
        #Break into lines, find line with table containing memory info, then tokenise the oine and get the cells containing the memory used and requested
        key = "Memory (MB)"
        logFileLines = logFileText.split("\n")
        for line in logFileLines :
            if key in line :
                tokens = line.split()
                if len(tokens) == 6 :
                  memoryUsed = int(tokens[3])
                  memoryRequested = int(tokens[4])
                  memoryAllocated = int(tokens[5])
                  return memoryUsed,memoryRequested,memoryAllocated

        #If here, didn't manage to parse
        return None,None,None


#
# Test
#

if __name__ == "__main__" :

    from .job import ClusterCommand, ClusterJob

    test_dir = "./tmp/condor"
    make_dir(test_dir)

    exe_path = "echo"
    exe_args = [
        "bar",
        "bar",
    ]

    dagman_file,condor_file = create_dagman_submit_file(submit_dir = test_dir,
                                                        log_dir = test_dir,
                                                        dagman_file_name = "test.dagman",
                                                        condor_file_name = "test.condor",
                                                        exe_path=exe_path,
                                                        exe_args=exe_args,
                                                        memory=1000)
